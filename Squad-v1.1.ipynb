{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f9944244-294c-42ad-90a5-d91b82058815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e36108-b04f-4f99-8c59-4e86a8982d52",
   "metadata": {},
   "source": [
    "### Scarichiamo i file SQUAD v1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "971e174a-b8b6-4d18-b324-1adc173af4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"\n",
    "dev = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
    "evaluate = \"https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5dee311f-4a2e-4840-9e0b-3fa7ca08387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./SQUAD/train-v1.1.json\"):\n",
    "    !wget $train && mv train-v1.1.json SQUAD/\n",
    "\n",
    "if not os.path.exists(\"./SQUAD/dev-v1.1.json\"):\n",
    "    !wget $dev && mv dev-v1.1.json SQUAD/\n",
    "\n",
    "if not os.path.exists(\"./SQUAD/evaluate-v1.1.py\"):\n",
    "    !wget $evaluate && mv evaluate-v1.1.py SQUAD/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6384a760-8319-4270-8fae-81a7c8a8d33d",
   "metadata": {},
   "source": [
    "### Carichiamoli in memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cd97f1c7-4508-464f-a382-312a5f6c8c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = json.load(open(\"./SQUAD/train-v1.1.json\"))\n",
    "dev_data =  json.load(open(\"./SQUAD/dev-v1.1.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49a3f35-0c10-4a4f-bc6b-86e6fe84b248",
   "metadata": {},
   "source": [
    "### Analizziamone la struttura\n",
    "1. Capiamo il tipo\n",
    "2. Se è un dict controlliamo le sottostruttre\n",
    "3. Se è una lista guardiamone la lunghezza ed il contenuto\n",
    "4. Se è stringa stampiamone il contenuto\n",
    "5. Per ogni Key,Values del punto 2 e per ogni oggetto del punto 3 ripetiamo il processo dal punto 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ea4aba82-291b-4076-86fe-4781f6e1b45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Train Stats ***\n",
      "Key: 'data', Value Type: <class 'list'>\n",
      "\tLength of values list: 442\n",
      "Key: 'version', Value Type: <class 'str'>\n",
      "\tValue: 1.1\n"
     ]
    }
   ],
   "source": [
    "print(\"*** Train Stats ***\")\n",
    "for k,v in train_data.items():\n",
    "    print(f\"Key: '{k}', Value Type: {type(v)}\")\n",
    "    if type(v) == dict:\n",
    "        print(f\"\\tValues keys: {v.keys()}\")\n",
    "    elif type(v) == list:\n",
    "        print(f\"\\tLength of values list: {len(v)}\")\n",
    "    else:\n",
    "        print(f\"\\tValue: {v}\")\n",
    "          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d5cc2bbc-2a4e-483a-8559-bff7a108cd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Dev Stats ***\n",
      "Key: 'data', Value Type: <class 'list'>\n",
      "\tLength of values list: 48\n",
      "Key: 'version', Value Type: <class 'str'>\n",
      "\tValue: 1.1\n"
     ]
    }
   ],
   "source": [
    "print(\"*** Dev Stats ***\")\n",
    "for k,v in dev_data.items():\n",
    "    print(f\"Key: '{k}', Value Type: {type(v)}\")\n",
    "    if type(v) == dict:\n",
    "        print(f\"\\tValues keys: {v.keys()}\")\n",
    "    elif type(v) == list:\n",
    "        print(f\"\\tLength of values list: {len(v)}\")\n",
    "    else:\n",
    "        print(f\"\\tValue: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "571ceb8b-0bc4-49ad-af41-0d61cdfaabde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'paragraphs'])\n"
     ]
    }
   ],
   "source": [
    "passage = train_data[\"data\"][0]\n",
    "print(passage.keys())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f9f91344-5738-4aa3-96f5-db4e38182515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University_of_Notre_Dame\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(passage[\"title\"])\n",
    "print(type(passage[\"paragraphs\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "acc252da-e902-4ec0-8c84-742e844b6dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = passage[\"paragraphs\"][0]\n",
    "type(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eb96557b-2828-4f0f-8e00-1939f825137f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['context', 'qas'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0583584f-4058-4a83-92d1-6705ff00ad6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(paragraph['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fad3628b-f2c9-4228-abfe-726928f00c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0a1a3c7c-4705-484c-a0b6-95a4a2f90379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(paragraph['qas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9084abe3-d16d-4387-98f7-b4066d18eb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = paragraph['qas'][0]\n",
    "type(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c7a462a5-4e9e-48bb-b389-c8ca3ee8aa7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['answers', 'question', 'id'])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8fb12890-9d32-4056-ad69-118e144ac869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5733be284776f41900661182\n"
     ]
    }
   ],
   "source": [
    "print(qa[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "56b4ec34-93df-41bc-acef-58d8f5dd32a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n"
     ]
    }
   ],
   "source": [
    "print(qa[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "20236244-2d74-4610-a70d-f441f7a18841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'answer_start': 515, 'text': 'Saint Bernadette Soubirous'}]\n"
     ]
    }
   ],
   "source": [
    "print(qa[\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3c6455ba-1101-4e16-8836-7b90e58901d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': str,\n",
       " 'data': [{'title': str,\n",
       "   'paragraphs': [{'context': str,\n",
       "     'qas': [{'id': str,\n",
       "       'question': str,\n",
       "       'answers': [{'answer_start': int, 'text': str}]}]}]}]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"version\": str,\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": str,\n",
    "            \"paragraphs\": [\n",
    "                {\n",
    "                    \"context\": str,\n",
    "                    \"qas\": [\n",
    "                        {\n",
    "                            \"id\": str,\n",
    "                            \"question\": str,\n",
    "                            \"answers\": [\n",
    "                                {\n",
    "                                    \"answer_start\": int,\n",
    "                                    \"text\": str\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                }    \n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dde607-0292-4e43-8480-a197a3d0dc30",
   "metadata": {},
   "source": [
    "### La struttura del Dev set è uguale al train quindi non rifacciamo l'esercizio e passiamo a rilevare delle statistiche sul dataset\n",
    "1. Quanti passaggi\n",
    "2. Quanti paragrafi in totale, per passaggio in media\n",
    "3. Lunghezza media/min/max di ogni passaggio in caratteri e token (un token è un pezzo di testo tra due spazi)\n",
    "4. Quante domande e quante risposte per ogni paragrafo in media\n",
    "5. Dato che abbiamo visto che le 'answers' sono di tipo lista, quante risposte ci sono in media/min/max per ogni domanda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bdf64e9c-fc4a-43da-ae27-789156732a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_stats(data_dict):\n",
    "    n_docs = 0\n",
    "    n_ans = 0\n",
    "    len_token = []\n",
    "    len_chars = []\n",
    "    par_list = []\n",
    "    q_list = []\n",
    "    q_to_context = {}\n",
    "    q_to_context_noanswers = {}\n",
    "    for doc in data_dict[\"data\"]:\n",
    "        n_docs += 1\n",
    "        for par in doc[\"paragraphs\"]:\n",
    "            par_list.append(par[\"context\"])\n",
    "            len_token.append(len(par[\"context\"].split(\" \")))\n",
    "            len_chars.append(len(par[\"context\"]))\n",
    "            for qa in par[\"qas\"]:\n",
    "                q_list.append(qa[\"question\"])\n",
    "                q_to_context[qa[\"question\"]] = par[\"context\"]\n",
    "                if len(qa[\"answers\"]) == 0:\n",
    "                    q_to_context_noanswers[qa[\"question\"]] = par[\"context\"]\n",
    "                for ans in qa[\"answers\"]:\n",
    "                    n_ans += 1\n",
    "                    \n",
    "    print(f\"Docs: {n_docs}\")\n",
    "    print(f\"Paragraphs: {len(par_list)}\")\n",
    "    print(f\"Q&A: {len(q_list)}\")\n",
    "    print(f\"Answers: {n_ans}\")\n",
    "    print(f\"Mean number of Paragraphs per Document: {round(len(par_list)/n_docs,2)}\")\n",
    "    print(f\"Min length in Tokens of a Paragraph: {np.min(len_token)}\")\n",
    "    print(f\"Max length in Tokens of a Paragraph: {np.max(len_token)}\")\n",
    "    print(f\"Mean length in Tokens of a Paragraph: {round(np.mean(len_token),2)}\")\n",
    "    print(f\"Min length in Chars of a Paragraph: {np.min(len_chars)}\")\n",
    "    print(f\"Max length in Chars of a Paragraph: {np.max(len_chars)}\")\n",
    "    print(f\"Mean length in Chars of a Paragraph: {round(np.mean(len_chars),2)}\")\n",
    "    return par_list, q_list, q_to_context, q_to_context_noanswers\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4a406be2-2c89-4ad7-91f4-a8a9529730e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Docs: 442\n",
      "Paragraphs: 18896\n",
      "Q&A: 87599\n",
      "Answers: 87599\n",
      "Mean number of Paragraphs per Document: 42.75\n",
      "Min length in Tokens of a Paragraph: 20\n",
      "Max length in Tokens of a Paragraph: 653\n",
      "Mean length in Tokens of a Paragraph: 116.63\n",
      "Min length in Chars of a Paragraph: 151\n",
      "Max length in Chars of a Paragraph: 3706\n",
      "Mean length in Chars of a Paragraph: 735.78\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data:\")\n",
    "train_par_list, train_q_list, train_q_to_context, _ = get_stats(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "de772d57-de1d-47c9-983d-8718c28888fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Data:\n",
      "Docs: 48\n",
      "Paragraphs: 2067\n",
      "Q&A: 10570\n",
      "Answers: 34726\n",
      "Mean number of Paragraphs per Document: 43.06\n",
      "Min length in Tokens of a Paragraph: 22\n",
      "Max length in Tokens of a Paragraph: 629\n",
      "Mean length in Tokens of a Paragraph: 122.76\n",
      "Min length in Chars of a Paragraph: 157\n",
      "Max length in Chars of a Paragraph: 4063\n",
      "Mean length in Chars of a Paragraph: 774.33\n"
     ]
    }
   ],
   "source": [
    "print(\"Dev Data:\")\n",
    "dev_par_list, dev_q_list, dev_q_to_context, _ = get_stats(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735b2a18-2fb5-4281-9804-5b9378cc150e",
   "metadata": {},
   "source": [
    "## Adesso che sappiamo un pò di più della struttura del dataset, vediamo qualitativamente che dati contiene\n",
    "Nelle prossime celle analizzeremo il sentiment dei paragrafi e delle domande contenute nel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e5473b7d-b85c-476c-84f2-7545e3ff1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "66ab6ff0-a0f5-478b-8927-6bbecfb74ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(paragraph_list):\n",
    "    classifier = pipeline('sentiment-analysis', framework=\"tf\", device=0)\n",
    "    sentiment_recap = {}\n",
    "    errors = []\n",
    "    for txt in tqdm(paragraph_list):\n",
    "        try:\n",
    "            sent = classifier(txt)[0]\n",
    "            label = sent[\"label\"]\n",
    "            if label not in sentiment_recap:\n",
    "                sentiment_recap[label] = 0\n",
    "            sentiment_recap[label] += 1\n",
    "        except Exception as e:\n",
    "            errors.append(e)\n",
    "    print(f\"Number of errors: {len(errors)}\")\n",
    "    if len(errors) > 0:\n",
    "        print(errors)\n",
    "    return sentiment_recap\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15c801c8-3dbc-4add-b30e-976cfaa34d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 499/18896 [00:15<09:34, 32.04it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 18896/18896 [10:01<00:00, 31.39it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sent_recap_par = sentiment_analysis(train_par_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77598275-9cdb-49bc-a42c-3407009f7a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Paragraph Sentiment Recap:\n",
      "{'POSITIVE': 9584, 'NEGATIVE': 9312}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Paragraph Sentiment Recap:\")\n",
    "print(train_sent_recap_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c5cf0ec-e943-4eef-adfd-aa04602113e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 36%|███▋      | 752/2067 [00:23<00:41, 31.31it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 2067/2067 [01:05<00:00, 31.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev_sent_recap_par = sentiment_analysis(dev_par_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01da2ca9-8c99-4987-8331-45e4f6511453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Paragraph Sentiment Recap:\n",
      "{'POSITIVE': 2067}\n"
     ]
    }
   ],
   "source": [
    "print(\"Dev Paragraph Sentiment Recap:\")\n",
    "print(dev_sent_recap_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "158f62a8-b5dd-4314-b85f-e51c30bf2be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def embedding_similarity(q_to_context, limit = 1000):\n",
    "    feature_extractor = pipeline('feature-extraction', framework=\"tf\", device=0)\n",
    "    q_to_context_similarity = []\n",
    "    question_keys = list(q_to_context.keys())\n",
    "    np.random.shuffle(question_keys)\n",
    "    selected_questions = question_keys[:limit]\n",
    "    for question in tqdm(selected_questions):\n",
    "        context = q_to_context[question]\n",
    "        q_embedding = np.mean(feature_extractor(question)[0],axis=0)\n",
    "        cntxt_embedding = np.mean(feature_extractor(context)[0],axis=0)\n",
    "        q_to_context_similarity.append(cosine_similarity([q_embedding], [cntxt_embedding])[0][0])\n",
    "        if len(q_to_context_similarity) >= limit:\n",
    "            break\n",
    "    \n",
    "    print(f\"Mean similarity between Question and Context: {np.mean(q_to_context_similarity)}\")\n",
    "    print(f\"Min similarity between Question and Context: {np.min(q_to_context_similarity)}\")\n",
    "    print(f\"Max similarity between Question and Context: {np.max(q_to_context_similarity)}\")\n",
    "    return q_to_context_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "09fba990-f520-48ef-bb83-218993d95e60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_projector', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n",
      "100%|█████████▉| 999/1000 [01:08<00:00, 14.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean similarity between Question and Context: 0.8687367180927811\n",
      "Min similarity between Question and Context: 0.5058140402280348\n",
      "Max similarity between Question and Context: 0.9578740581088679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q_to_context_similarity_train = embedding_similarity(train_q_to_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "34fd60b6-7629-4665-9b51-056f6fce383c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_projector', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n",
      " 11%|█▏        | 114/1000 [00:07<01:02, 14.27it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████▉| 999/1000 [01:08<00:00, 14.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean similarity between Question and Context: 0.8726055091638873\n",
      "Min similarity between Question and Context: 0.6852343126241149\n",
      "Max similarity between Question and Context: 0.9556624586384194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q_to_context_similarity_train = embedding_similarity(dev_q_to_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e08106-b468-4e0d-b899-43820d7fdc5f",
   "metadata": {},
   "source": [
    "### SQUAD v2.0 \n",
    "Adesso analizziamo la distanza di embedding tra le domande senza risposta ed i loro rispettivi passaggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "faff251e-37f2-4d40-8964-bb0d1942c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train_v2 = json.load(open(\"./train-v2.0.json\"))\n",
    "squad_dev_v2 = json.load(open(\"./dev-v2.0.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "80fa0834-c0a6-410b-ade7-47dc58cd82ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 442\n",
      "Paragraphs: 19035\n",
      "Q&A: 130319\n",
      "Answers: 86821\n",
      "Mean number of Paragraphs per Document: 43.07\n",
      "Min length in Tokens of a Paragraph: 20\n",
      "Max length in Tokens of a Paragraph: 653\n",
      "Mean length in Tokens of a Paragraph: 116.59\n",
      "Min length in Chars of a Paragraph: 151\n",
      "Max length in Chars of a Paragraph: 3706\n",
      "Mean length in Chars of a Paragraph: 735.55\n"
     ]
    }
   ],
   "source": [
    "train_v2_par_list, train_v2_q_list, train_v2_q_to_context, train_v2_q_to_context_noanswers = get_stats(squad_train_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "862639f1-dac8-4b58-8cb1-355a21faf9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 35\n",
      "Paragraphs: 1204\n",
      "Q&A: 11873\n",
      "Answers: 20302\n",
      "Mean number of Paragraphs per Document: 34.4\n",
      "Min length in Tokens of a Paragraph: 25\n",
      "Max length in Tokens of a Paragraph: 629\n",
      "Mean length in Tokens of a Paragraph: 126.54\n",
      "Min length in Chars of a Paragraph: 169\n",
      "Max length in Chars of a Paragraph: 4063\n",
      "Mean length in Chars of a Paragraph: 802.61\n"
     ]
    }
   ],
   "source": [
    "dev_v2_par_list, dev_v2_q_list, dev_v2_q_to_context, dev_v2_q_to_context_noanswers = get_stats(squad_dev_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d2eef225-224d-43fb-aded-dda880b51b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_projector', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n",
      " 50%|█████     | 504/1000 [00:34<00:33, 14.81it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████▉| 999/1000 [01:08<00:00, 14.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean similarity between Question and Context: 0.8620405550912449\n",
      "Min similarity between Question and Context: 0.6554168797417589\n",
      "Max similarity between Question and Context: 0.9529050336541132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q_to_context_similarity_noanswer_train_v2 = embedding_similarity(train_v2_q_to_context_noanswers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6083a29d-79f2-4757-a93d-d60ae11c5f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_projector', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n",
      " 14%|█▍        | 144/1000 [00:09<00:57, 14.93it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (659 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████▉| 999/1000 [01:08<00:00, 14.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean similarity between Question and Context: 0.8638132165284282\n",
      "Min similarity between Question and Context: 0.5489242513107354\n",
      "Max similarity between Question and Context: 0.9522213888134166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q_to_context_similarity_noanswer_dev_v2 = embedding_similarity(dev_v2_q_to_context_noanswers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c12805-1f2d-4606-81d2-99d61aeb350b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
